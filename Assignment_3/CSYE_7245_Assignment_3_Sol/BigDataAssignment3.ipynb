{"cells":[{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport pyspark.sql\nimport pyspark.ml"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["df=spark.read.csv('/FileStore/tables/News_Data.csv',header=True,inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["df = df.filter(\"CATEGORY != ''\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df.head()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["display(df.limit(5))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df.createOrReplaceTempView(\"tblnews\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql select * from tblnews"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%sql select TITLE, PUBLISHER, CATEGORY from tblnews "],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#group by is used to show the number of the articles in a category"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%sql select count(TITLE), CATEGORY from tblnews group by CATEGORY order by count(TITLE) desc "],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#group by is used to show the count of the publications"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%sql select count(TITLE), PUBLISHER from tblnews group by PUBLISHER order by count(TITLE) desc"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#Self join is used to show the title of the article which were written by the same PUBLISHER but not same"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%sql SELECT A.TITLE, B.TITLE, A.PUBLISHER\nFROM tblnews A, tblnews B\nWHERE A.TITLE <> B.TITLE\nAND A.PUBLISHER = B.PUBLISHER\norder by A.PUBLISHER desc\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#Having clause to show only count which is greater than 1000"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sql select count(TITLE), PUBLISHER from tblnews group by PUBLISHER having count(TITLE)>1000 "],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#the name of the PUBLISHER who has written maximum no of articles"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%sql select  count(TITLE), PUBLISHER from tblnews group by PUBLISHER order by count(TITLE) desc LIMIT 1"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["#the name of the category which has maximum no of articles"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%sql select count(TITLE), CATEGORY from tblnews group by CATEGORY order by count(TITLE) desc Limit 1"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["#select the Publisher and Category name whose name starts with alphabet a"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%sql select PUBLISHER, CATEGORY from tblnews WHERE PUBLISHER LIKE 'a%'"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#Select Publication between the two given TIMESTAMP"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%sql select PUBLISHER,CATEGORY from tblnews where TIMESTAMP between '1.39447E+12' AND '1.40268E+12'"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# Machine Learning "],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n# Convert target into numerical categories\nlabelIndexer = StringIndexer(inputCol=\"CATEGORY\", outputCol=\"label\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["print(labelIndexer)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["#Dividing dataset into training and testing \n(trainingData, testData) = df.randomSplit([0.7, 0.3], seed = 100)\n\ntrainingData.cache()\ntestData.cache()\n\nprint (trainingData.count())\nprint (testData.count())"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["trainingData"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml import Pipeline\n\n# Train a NaiveBayes model\nnb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n\n# Chain labelIndexer, vecAssembler and NBmodel in a \npipeline = Pipeline(stages=[labelIndexer, nb])"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["from pyspark.ml import *\nfrom pyspark.ml.classification import *\nfrom pyspark.ml.feature import *\nfrom pyspark.ml.param import *\nfrom pyspark.ml.tuning import *\nfrom pyspark.ml.evaluation import *"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["indexer = StringIndexer(inputCol=\"CATEGORY\", outputCol=\"label\")\nindexed = indexer.fit(trainingData).transform(trainingData)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["labelIndexer = StringIndexer(inputCol=\"CATEGORY\", outputCol=\"label\")"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Constructing a pipeline is done by creating each pipeline stage and configuing its parameters.\ntokenizer = RegexTokenizer(inputCol=\"TITLE\", outputCol=\"words\", pattern=\"s+\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\", numFeatures=5000)\nlr = LogisticRegression(maxIter=20, regParam=0.01)\n\n# To create an ML pipeline you concatenate a sequence of stages.\npipeline = Pipeline(stages=[labelIndexer,tokenizer, hashingTF, lr])"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# Run stages in pipeline and train model\nmodel = pipeline.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# Make predictions on testData so we can measure the accuracy of our model on new data\npredictions = model.transform(testData)\n\n# Display what results we can view\npredictions.printSchema()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["display(predictions.select(\"label\", \"prediction\", \"probability\"))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["predictions"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# After fitting, making predictions is as simple as calling \"transform\" on the model.\nprediction = model.transform(trainingData)\n# Show the predicted labels along with true labels and raw texts.\ndisplay(prediction.select(\"prediction\", \"label\", \"TITLE\").limit(10))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\nevaluator.evaluate(prediction)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["from pyspark.mllib.evaluation import MulticlassMetrics\n# Create (prediction, label) pairs\npredictionAndLabel = predictions.select(\"prediction\", \"label\").rdd\n\n# Generate confusion matrix\nmetrics = MulticlassMetrics(predictionAndLabel)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["#GraphX and GraphFrames analysis"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["%scala\nimport org.apache.spark._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.graphx._"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["df5=spark.read.csv('/FileStore/tables/vertex_edge.csv',header=True,inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["from graphframes import *\n\nfrom pyspark.sql.functions import monotonically_increasing_id \n\ndf = df.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n\ng = GraphFrame(df, df5)\nprint (g)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["results = g.pageRank(resetProbability=0.15, tol=0.01)\ndisplay(results.vertices)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["display(results.edges)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["g.pageRank(resetProbability=0.15, maxIter=10)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["# Run PageRank personalized for vertex \"a\"\ng.pageRank(resetProbability=0.15, maxIter=10, sourceId=\"1\")"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["results = g.shortestPaths(landmarks=[\"1\", \"10\"])\ndisplay(results)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["result = g.labelPropagation(maxIter=5)\ndisplay(result)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["result = g.stronglyConnectedComponents(maxIter=10)\ndisplay(result.select(\"id\", \"component\"))"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["numFollows = g.edges.filter(\"relationship = 'same_category'\").count()\nprint(\"The number of follow edges is\", numFollows)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["numFollows = g.edges.filter(\"relationship = 'same_publisher'\").count()\nprint(\"The number of follow edges is\", numFollows)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":57}],"metadata":{"name":"BigDataAssignment3","notebookId":2099574030447960},"nbformat":4,"nbformat_minor":0}
